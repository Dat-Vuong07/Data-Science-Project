---
title: "Work Flow Data Science Project"
date: "5/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Package

```{r Packages, message=FALSE}

library(tidyverse) # data manipulation
library(quanteda) # text pre-processing
library(tidytext) # text analysis
library(lubridate) # date function
library(gridExtra) # multiple graphs
library(ggrepel) # plot labels
library(stm) # topic modelling
library(stmCorrViz)
library(furrr)
library(tm)

```

# Data Collection

Using the keyword "coronavirus" for querying, we collected news articles published on news websites in the United States from the beginning of 2020, where the first cases of coronavirus were reported, until April 19, 2020. We used a third-party API called Currents API which generated data in JSON format. For sources, we looked at the top 15 U.S news websites measured by unique monthly visitors (Statista, 2020), excluding the news aggregators (Yahoo News, Google News) and topic-specific newspapers (The Wall Street Journal). The free version of the API, despite returning some useful features including Title, Date, Description and URL, did not allow us getting the full text of the articles. Therefore, we found a resolution by putting all the URLs available from the previous step into the newspaper3k module in Python, which enabled scraping the necessary text. Furthermore, after checking the data quality from each source, we narrowed them down to these 6 news outlets: CNN, The New York Times, Fox News, The Guardian, USA Today and The LA Times.

For Reddit data, we searched for all comments from the subreddit CoronavirusUS from February when it was created until April 19, 2020. We used the pushshift.io Reddit API to build the URL with the relevant parameters which returned a page of JSON objects.

In view of Twitter data, the standard API only allows search queries of up to 7 days which means that we cannot scrape data in the normal way for more historical data. However, thanks to the support from the team at crowdbreaks.org, who is tracking Twitter trend on COVID-19 in real time, we managed to get all tweet IDs that they had collected since January in the U.S. By creating a Python environment and executing the download command following their guidelines, we retrieved the tweets from the provided IDs, also in JSON format.

We used both the jsonlite package in R and the json module in Python to extract key features from each JSON result, namely the date and text attributes. Unfortunately, because of Twitter's restriction allowing only 180 tweets every 15 minutes, we calculated a total download time of 15 days using 2 API keys we had. On that account, we decided to split the data in half by stratified sampling, keeping the distribution of number of tweets per week, in order to speed up data collection and obtain the data in time for the report. The Twitter data were collected until April 16, 2020.

# Flow 1 - Load + Clean data

## Reddit

```{r, message=FALSE}

reddit_data <- read_rds("Reddit_Data.rds")

```

## Twitter

```{r, message=FALSE}

tweets_data <- read_csv("twitter_coronavirus.csv")

# Transform the date column

tweets_data <- tweets_data %>% 
  separate(created_at, c("wday", "month", "day", "time", "plus", "year"), 
           sep = " ") %>% 
  mutate(date = paste(month, day, year, sep = " ")) %>% 
  select(c("text", "date"))

tweets_data$date <- as.Date(tweets_data$date, format = "%b %d %Y")
tweets_data <- tweets_data %>% 
  mutate(week = isoweek(date)) %>% 
  filter(week > 2)

# Clean hashtags, links and special characters

tweets_data$text <- tweets_data$text %>% 
  str_replace_all("#", " ") %>%
  str_remove_all("(?<=^|\\s)http[^\\s]+") %>% 
  str_remove_all("[^a-zA-Z0-9 ]") %>% 
  trimws()

# Remove blank text after cleaning

tweets_data <- tweets_data %>% 
  filter(str_count(text, pattern = boundary("word")) > 1)
```


## News

```{r, message=FALSE}

news_data <- read_csv("news_coronavirus.csv")

# Add week and remove unnecessary column

news_data <- news_data %>% 
  mutate(week = isoweek(publish_date)) %>% 
  select(-title)

```

For data cleaning, we removed duplicated data, texts that were too short and kept only texts written in English. Consequently, 14,149 news articles, 352,842 Reddit comments and 109,329 tweets were ready for analysis. The data distribution can be seen in Figure 1. The number of news reported on COVID-19 in the U.S were relatively low in the first 6 weeks (week 3 - week 8), and the subreddit CoronavirusUS was not even created until week 7. However, there was a dramatic increase starting from week 9 and peaked in week 11, 12 and 13 for Twitter, Reddit and News respectively. This was the end of March when the number of infected cases in the country surged.

```{r Data Count, echo=FALSE}

plot_news <- news_data %>%
  group_by(week) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(source = "News")

plot_tweets <- tweets_data %>%
  group_by(week) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(source = "Twitter")

plot_reddit <- reddit_data %>%
  group_by(week) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(source = "Reddit")

```

```{r Data Distribution, echo=FALSE}

news_distribution <- plot_news %>%
  ggplot(aes(x = week)) + 
  geom_col(aes(y = n, fill = -n), show.legend = FALSE) +
  scale_x_continuous(n.breaks = 10) +
  theme(panel.border = element_rect("lightgray", fill = NA)) +
  labs(x = NULL, y = NULL, caption = "News") +
  theme(
    axis.text.x = element_text(size = 14, hjust = .5, vjust = .5),
    axis.text.y = element_text(size = 14),
    plot.caption = element_text(hjust = 0.5, size = rel(1.8)))

tweets_distribution <- plot_tweets %>%
  ggplot(aes(x = week)) + 
  geom_col(aes(y = n, fill = -n), show.legend = FALSE) +
  scale_x_continuous(n.breaks = 10) +
  theme(panel.border = element_rect("lightgray", fill = NA)) +
  labs(x = NULL, y = NULL, caption = "Twitter") +
  theme(
    axis.text.x = element_text(size = 14, hjust = .5, vjust = .5),
    axis.text.y = element_text(size = 14),
    plot.caption = element_text(hjust = 0.5, size = rel(1.8)))

reddit_distribution <- plot_reddit %>%
  ggplot(aes(x = week)) + 
  geom_col(aes(y = n, fill = -n), show.legend = FALSE) +
  scale_x_continuous(n.breaks = 6) +
  theme(panel.border = element_rect("lightgray", fill = NA)) +
  labs(x = NULL, y = NULL, caption = "Reddit") +
  theme(
    axis.text.x = element_text(size = 14, hjust = .5, vjust = .5),
    axis.text.y = element_text(size = 14),
    plot.caption = element_text(hjust = 0.5, size = rel(1.8)))

grid.arrange(news_distribution, tweets_distribution, reddit_distribution,
             ncol = 3)

```


# Flow 2 - Corpus - Token

For text data preparation, we focus on quanteda package in R, which is one of the most popular R packages for the quantitative analysis of textual data. Firstly, we created a corpus for the data set of each source. A corpus consists of a collection of documents (each document is an article, a Reddit comment or a tweet), and the document variables which describe the characteristics of the document, for instance published date and source. Subsequently, we tokenized each corpus, which separates the text into into its single words (also called terms or tokens). We pre-processed the data sets by transforming terms into lowercase and removing the numbers, punctuations, symbols, URLs and separators. We also implemented  stopword removal and lemmatization technique. Stopwords are words that appear in texts but do not give the text a substantial meaning (e.g., "the", "a", or "for") and lemmatization deals with the inflected forms of words by replacing them with their base forms.

In the next step, we generated a document-feature matrix (also known as document-term matrix) for each source. It represents how frequently terms occur in the corpus by counting single terms. We kept only the top 5% of the most frequent features (minimum term frequency set at 0.95) that present in less than 10% of all documents (maximum document frequency set at 0.1) to focus on common but distinctive features.

## Reddit

### Create a corpus 

```{r}

corpus_Reddit_data <- corpus(reddit_data, text_field = c("comments"), unique_docnames = F) %>%
  `docnames<-`(reddit_data$week)

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")
```


### Create Token

```{r}

Token_Reddit_data <- corpus_Reddit_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_remove(pattern = c("subreddit", "subredditmessagecomposetorcoronavirusus", "652000", "discussion", "thread", "subreddits", "people")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

```{r}

Reddit_DFM <- Token_Reddit_data %>% 
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

Reddit_DFM <- Reddit_DFM[ntoken(Reddit_DFM) > 0,]
```


## Twitter

### Create a corpus 

```{r}

corpus_Twitter_data <- corpus(tweets_data, text_field = c("text"),
                             unique_docnames = F) %>%
  `docnames<-`(tweets_data$week)

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")
```


### Create Token

```{r}

Token_Twitter_data <- corpus_Twitter_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = FALSE) %>%
  tokens_remove(pattern = c("people", "amp", "rt", "im", "dont", "ha")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, 
                 valuetype = "glob") %>% 
  tokens_replace(pattern = c("covid-19", "covid19", "covid"), 
                 c("coronavirus", "coronavirus", "coronavirus"),
                 valuetype = "fixed") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

```{r}

Twitter_DFM <- Token_Twitter_data %>% 
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

Twitter_DFM <- Twitter_DFM[ntoken(Twitter_DFM) > 0,]
```


## News

### Create a corpus 

```{r}

corpus_News_data <- corpus(news_data, text_field = c("text"), 
                           unique_docnames = F) %>%
  `docnames<-`(news_data$week) # update the name of document in the corpus
#head(corpus_News_data, 5) # take a look at the corpus
```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")

```

### Create Tokens

```{r}

Token_news_data <- corpus_News_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, 
         remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = FALSE) %>%
  tokens_remove(pattern = c("amid", "updates", "live", "video", 
                            "didn", "briefing")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, 
                 valuetype = "glob") %>%
  tokens_replace(pattern = c("covid-19", "ncov", "cov"), 
                 c("coronavirus", "coronavirus", "coronavirus"),
                 valuetype = "fixed") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

```{r}

News_DFM <- Token_news_data %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

News_DFM <- News_DFM[ntoken(News_DFM) > 0, ]
#head(News_DFM, n = 5, nf = 8)

```


# Flow 3 - Co-occurrence analysis

`Source: https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html`

## Reddit Network

### Topic-specific dictionary

```{r}

Corona_words <- Token_reddit_data %>%
  kwic(., pattern =  'corona*', window = 4) %>%
  as.data.frame() %>%
  distinct(keyword)

covid <- Token_reddit_data %>%
  kwic(., pattern =  'covid*', window = 4) %>%
  as.data.frame() %>%
  distinct(keyword)

ncov <- Token_reddit_data %>%
  kwic(., pattern =  'ncov*', window = 4) %>%
  as.data.frame() %>%
  distinct(keyword)

Corona_Dictionary <- read_csv("Corona_Dictionary.csv") %>%
  mutate(replace = "coronavirus")

```

### Load Token & Dictionary

```{r}
Corona_Dictionary <- read_csv("Corona_Dictionary2.csv")
  

Token_reddit_network <- corpus_Reddit_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_replace(Corona_Dictionary$keyword, Corona_Dictionary$replace) %>% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T) %>%
  tokens_remove(pattern = c("subreddit", "subredditmessagecomposetorcoronavirusus", 
                            "652000", "discussion", "thread", "subreddits", "people",
                            "gt"))

```

### Load the DFM into the data

Again, we create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. An upper limit is not set (`Inf` = infinite).

Additionally, we are interested in the joint occurrence of words in a sentence. For this, we do not need the exact count of how often the terms occur, but only the information whether they occur together or not. This can be encoded in a binary document-term-matrix. The parameter `weighting` in the control options calls the `weightBin` function. This writes a 1 into the DTM if the term is contained in a sentence and 0 if not.

```{r}
# calculate multi-word unit candidates
Redit_collocations <- textstat_collocations(Token_reddit_network, min_count = 50)

Redit_collocations <- tokens_compound(Token_reddit_network, Redit_collocations)

minimumFrequency <- 10

Reddit_binDTM <- Redit_collocations %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")
```

### Counting co-occurrences

The counting of the joint word occurrence is easily possible via a matrix multiplication `(https://en.wikipedia.org/wiki/Matrix_multiplication)` on the binary DTM. For this purpose, the transposed matrix (dimensions: nTypes x nDocs) is multiplied by the original matrix (nDocs x nTypes), which as a result encodes a term-term matrix (dimensions: nTypes x nTypes).

```{r}
# Matrix multiplication for cooccurrence counts

Reddit_coocCounts <- t(Reddit_binDTM) %*% Reddit_binDTM

```

Let’s look at a snippet of the result. The matrix has nTerms rows and columns and is symmetric. Each cell contains the number of joint occurrences. In the diagonal, the frequencies of single occurrences of each term are encoded.

```{r}
as.matrix(coocCounts[202:205, 202:205])
```

Interprete as follows: agree appears together 7 times with opinion in the 62735 sentences of the SUTO addresses. agree alone occurs 310 times.

### Statistical significance

In order to not only count joint occurrence we have to determine their significance. Different significance-measures can be used. We need also various counts to calculate the significance of the joint occurrence of a term `i` (`coocTerm`) with any other term `j`: 

  * `k` - Number of all context units in the corpus 
  * `ki` - Number of occurrences of `coocTerm` 
  * `kj` - Number of occurrences of comparison term `j` 
  * `kij` - Number of joint occurrences of `coocTerm` and `j`

These quantities can be calculated for any term `coocTerm` as follows:

```{r}
Reddit_coocTerm <- c("coronavirus")
Reddit_k <- nrow(Reddit_binDTM)
Reddit_ki <- sum(Reddit_binDTM[, Reddit_coocTerm])
Reddit_kj <- colSums(Reddit_binDTM)
names(Reddit_kj) <- colnames(Reddit_binDTM)
Reddit_kij <- Reddit_coocCounts[Reddit_coocTerm, ]
```


An implementation in R for Mutual Information, Dice, and Log-Likelihood may look like this. At the end of each formula, the result is sorted so that the most significant co-occurrences are at the first ranks of the list.

```{r}
########## MI: log(Reddit_k*Reddit_kij / (Reddit_ki * Reddit_kj) ########
mutualInformationSig <- log(Reddit_k * Reddit_kij / (Reddit_ki * Reddit_kj))
mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]

########## DICE: 2 X&Y / X + Y ##############
dicesig <- 2 * Reddit_kij / (Reddit_ki + Reddit_kj)
dicesig <- dicesig[order(dicesig, decreasing=TRUE)]

########## Log Likelihood ###################
logsig <- 2 * ((Reddit_k * log(Reddit_k)) - (Reddit_ki * log(Reddit_ki)) - (Reddit_kj * log(Reddit_kj)) + (Reddit_kij * log(Reddit_kij)) 
               + (Reddit_k - Reddit_ki - Reddit_kj + Reddit_kij) * log(Reddit_k - Reddit_ki - Reddit_kj + Reddit_kij) 
               + (Reddit_ki - Reddit_kij) * log(Reddit_ki - Reddit_kij) + (Reddit_kj - Reddit_kij) * log(Reddit_kj - Reddit_kij) 
               - (Reddit_k - Reddit_ki) * log(Reddit_k - Reddit_ki) - (Reddit_k - Reddit_kj) * log(Reddit_k - Reddit_kj))


logsig <- logsig[order(logsig, decreasing=T)]
```

The result of the four variants for the statistical extraction of co-occurrence terms is shown in a data frame below. It can be seen that frequency is a bad indicator of meaning constitution. Mutual information emphasizes rather rare events in the data. Dice and Log-likelihood yield very well interpretable contexts.

```{r}
# Put all significance statistics in one Data-Frame
Reddit_resultOverView <- data.frame(
  names(sort(Reddit_kij, decreasing=T)[1:10]), sort(Reddit_kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)

colnames(Reddit_resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")

print(Reddit_resultOverView)
```

### Visualization of co-occurrence

In the following, we create a network visualization of significant co-occurrences.

For this, we provide the calculation of the co-occurrence significance measures, which we have just introduced, as single function in the file `calculateCoocStatistics.R`. This function can be imported into the current R-Session with the source command.


```{r}
# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
Reddit_coocTerm <- "coronavirus"
```

We use the imported function `calculateCoocStatistics` to calculate the co-occurrences for the target term “coronavirus”.

```{r}
Reddit_coocs <- calculateCoocStatistics(Reddit_coocTerm, Reddit_binDTM, measure="LOGLIK")

# Display the numberOfCoocs main terms
print(Reddit_coocs[1:numberOfCoocs])
```

To acquire an extended semantic environment of the target term, ‘secondary co-occurrence’ terms can be computed for each co-occurrence term of the target term. This results in a graph that can be visualized with special layout algorithms (e.g. Force Directed Graph).

Network graphs can be evaluated and visualized in R with the igraph-package. Any graph object can be created from a three-column data-frame. Each row in that data-frame is a triple. Each triple encodes an edge-information of two nodes (source, sink) and an edge-weight value.

For a term co-occurrence network, each triple consists of the target word, a co-occurring word and the significance of their joint occurrence. We denote the values with from, to, sig.

```{r}

Reddit_resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

```

The process of gathering the network for the target term runs in two steps. First, we obtain all significant co-occurrence terms for the target term. Second, we obtain all co-occurrences of the co-occurrence terms from step one.

Intermediate results for each term are stored as temporary triples named `tmpGraph.` With the `rbind` command (“row bind”, used for concatenation of data-frames) all `tmpGraph` are appended to the complete network object stored in `resultGraph.`

```{r}
# The structure of the temporary graph object is equal to that of the Reddit_resultGraph
Reddit_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
Reddit_tmpGraph[1:numberOfCoocs, 3] <- Reddit_coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
Reddit_tmpGraph[, 1] <- Reddit_coocTerm
# Entry of the co-occurrences into the second column of the respective line
Reddit_tmpGraph[, 2] <- names(Reddit_coocs)[1:numberOfCoocs]
# Set the significances
Reddit_tmpGraph[, 3] <- Reddit_coocs[1:numberOfCoocs]

# Attach the triples to Reddit_resultGraph
Reddit_resultGraph <- rbind(Reddit_resultGraph, Reddit_tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newReddit_coocTerm <- names(Reddit_coocs)[i]
  Reddit_coocs2 <- calculateCoocStatistics(newReddit_coocTerm, Reddit_binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  Reddit_coocs2[1:10]
  
  # Structure of the temporary graph object
  Reddit_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  Reddit_tmpGraph[1:numberOfCoocs, 3] <- Reddit_coocs2[1:numberOfCoocs]
  Reddit_tmpGraph[, 1] <- newReddit_coocTerm
  Reddit_tmpGraph[, 2] <- names(Reddit_coocs2)[1:numberOfCoocs]
  Reddit_tmpGraph[, 3] <- Reddit_coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  Reddit_resultGraph <- rbind(Reddit_resultGraph, Reddit_tmpGraph[2:length(Reddit_tmpGraph[, 1]), ])
}
```

As a result, resultGraph now contains all numberOfCoocs * numberOfCoocs edges of a term co-occurrence network.

### Visualization 

The package iGraph offers multiple graph visualizations for graph objects. Graph objects can be created from triple lists, such as those we just generated. In the next step we load the package iGraph and create a visualization of all nodes and edges from the object resultGraph.

```{r}
require(igraph)

# Set the graph and type
Reddit_graphNetwork <- graph.data.frame(Reddit_resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
Reddit_graphVs <- V(Reddit_graphNetwork)[degree(Reddit_graphNetwork) < 2]
# These edges are removed from the graph
Reddit_graphNetwork <- delete.vertices(Reddit_graphNetwork, Reddit_graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(Reddit_graphNetwork)$color <- ifelse(V(Reddit_graphNetwork)$name == Reddit_coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
Reddit_halfMaxSig <- max(E(Reddit_graphNetwork)$sig) * 0.5
E(Reddit_graphNetwork)$color <- ifelse(E(Reddit_graphNetwork)$sig > Reddit_halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(Reddit_graphNetwork)$curved <- 0
# Size the nodes by their degree of networking
V(Reddit_graphNetwork)$size <- log(degree(Reddit_graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(Reddit_graphNetwork)$size[V(Reddit_graphNetwork)$size < 5] <- 4

# edge thickness
E(Reddit_graphNetwork)$width <- 1

# Finaler Plot
Reddit_network <- toVisNetworkData(Reddit_graphNetwork)
visNetwork(nodes = Reddit_network$nodes %>% 
             mutate(font.size = if_else(id == "coronavirus",25,15)),
           
           edges = Reddit_network$edges %>% 
             mutate(color = NA), 
           
           main = "Corona keyword Co-occurrence", 
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE)

  
# visSave(file = "network.html")
```



## News Network

### Load Token & Dictionary

```{r}
Corona_Dictionary <- read_csv("Corona_Dictionary2.csv")
  

Token_News_network <- corpus_News_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_replace(Corona_Dictionary$keyword, Corona_Dictionary$replace) %>% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T) %>%
  tokens_remove(pattern = c("subNews", "subNewsmessagecomposetorcoronavirusus", 
                            "652000", "discussion", "thread", "subNewss", "people",
                            "gt"))

```

### Load the DFM into the data

Again, we create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. An upper limit is not set (`Inf` = infinite).

Additionally, we are interested in the joint occurrence of words in a sentence. For this, we do not need the exact count of how often the terms occur, but only the information whether they occur together or not. This can be encoded in a binary document-term-matrix. The parameter `weighting` in the control options calls the `weightBin` function. This writes a 1 into the DTM if the term is contained in a sentence and 0 if not.

```{r}
# calculate multi-word unit candidates
Redit_collocations <- textstat_collocations(Token_News_network, min_count = 50)

Redit_collocations <- tokens_compound(Token_News_network, Redit_collocations)

minimumFrequency <- 10

News_binDTM <- Redit_collocations %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")
```

### Counting co-occurrences

The counting of the joint word occurrence is easily possible via a matrix multiplication `(https://en.wikipedia.org/wiki/Matrix_multiplication)` on the binary DTM. For this purpose, the transposed matrix (dimensions: nTypes x nDocs) is multiplied by the original matrix (nDocs x nTypes), which as a result encodes a term-term matrix (dimensions: nTypes x nTypes).

```{r}
# Matrix multiplication for cooccurrence counts

News_coocCounts <- t(News_binDTM) %*% News_binDTM

```

Let’s look at a snippet of the result. The matrix has nTerms rows and columns and is symmetric. Each cell contains the number of joint occurrences. In the diagonal, the frequencies of single occurrences of each term are encoded.

```{r}
as.matrix(coocCounts[202:205, 202:205])
```

Interprete as follows: agree appears together 7 times with opinion in the 62735 sentences of the SUTO addresses. agree alone occurs 310 times.

### Statistical significance

In order to not only count joint occurrence we have to determine their significance. Different significance-measures can be used. We need also various counts to calculate the significance of the joint occurrence of a term `i` (`coocTerm`) with any other term `j`: 

  * `k` - Number of all context units in the corpus 
  * `ki` - Number of occurrences of `coocTerm` 
  * `kj` - Number of occurrences of comparison term `j` 
  * `kij` - Number of joint occurrences of `coocTerm` and `j`

These quantities can be calculated for any term `coocTerm` as follows:

```{r}
News_coocTerm <- c("coronavirus")
News_k <- nrow(News_binDTM)
News_ki <- sum(News_binDTM[, News_coocTerm])
News_kj <- colSums(News_binDTM)
names(News_kj) <- colnames(News_binDTM)
News_kij <- News_coocCounts[News_coocTerm, ]
```


An implementation in R for Mutual Information, Dice, and Log-Likelihood may look like this. At the end of each formula, the result is sorted so that the most significant co-occurrences are at the first ranks of the list.

```{r}
########## MI: log(News_k*News_kij / (News_ki * News_kj) ########
mutualInformationSig <- log(News_k * News_kij / (News_ki * News_kj))
mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]

########## DICE: 2 X&Y / X + Y ##############
dicesig <- 2 * News_kij / (News_ki + News_kj)
dicesig <- dicesig[order(dicesig, decreasing=TRUE)]

########## Log Likelihood ###################
logsig <- 2 * ((News_k * log(News_k)) - (News_ki * log(News_ki)) - (News_kj * log(News_kj)) + (News_kij * log(News_kij)) 
               + (News_k - News_ki - News_kj + News_kij) * log(News_k - News_ki - News_kj + News_kij) 
               + (News_ki - News_kij) * log(News_ki - News_kij) + (News_kj - News_kij) * log(News_kj - News_kij) 
               - (News_k - News_ki) * log(News_k - News_ki) - (News_k - News_kj) * log(News_k - News_kj))


logsig <- logsig[order(logsig, decreasing=T)]
```

The result of the four variants for the statistical extraction of co-occurrence terms is shown in a data frame below. It can be seen that frequency is a bad indicator of meaning constitution. Mutual information emphasizes rather rare events in the data. Dice and Log-likelihood yield very well interpretable contexts.

```{r}
# Put all significance statistics in one Data-Frame
News_resultOverView <- data.frame(
  names(sort(News_kij, decreasing=T)[1:10]), sort(News_kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)

colnames(News_resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")

print(News_resultOverView)
```

### Visualization of co-occurrence

In the following, we create a network visualization of significant co-occurrences.

For this, we provide the calculation of the co-occurrence significance measures, which we have just introduced, as single function in the file `calculateCoocStatistics.R`. This function can be imported into the current R-Session with the source command.


```{r}
# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
News_coocTerm <- "coronavirus"
```

We use the imported function `calculateCoocStatistics` to calculate the co-occurrences for the target term “coronavirus”.

```{r}
News_coocs <- calculateCoocStatistics(News_coocTerm, News_binDTM, measure="LOGLIK")

# Display the numberOfCoocs main terms
print(News_coocs[1:numberOfCoocs])
```

To acquire an extended semantic environment of the target term, ‘secondary co-occurrence’ terms can be computed for each co-occurrence term of the target term. This results in a graph that can be visualized with special layout algorithms (e.g. Force Directed Graph).

Network graphs can be evaluated and visualized in R with the igraph-package. Any graph object can be created from a three-column data-frame. Each row in that data-frame is a triple. Each triple encodes an edge-information of two nodes (source, sink) and an edge-weight value.

For a term co-occurrence network, each triple consists of the target word, a co-occurring word and the significance of their joint occurrence. We denote the values with from, to, sig.

```{r}

News_resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

```

The process of gathering the network for the target term runs in two steps. First, we obtain all significant co-occurrence terms for the target term. Second, we obtain all co-occurrences of the co-occurrence terms from step one.

Intermediate results for each term are stored as temporary triples named `tmpGraph.` With the `rbind` command (“row bind”, used for concatenation of data-frames) all `tmpGraph` are appended to the complete network object stored in `resultGraph.`

```{r}
# The structure of the temporary graph object is equal to that of the News_resultGraph
News_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
News_tmpGraph[1:numberOfCoocs, 3] <- News_coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
News_tmpGraph[, 1] <- News_coocTerm
# Entry of the co-occurrences into the second column of the respective line
News_tmpGraph[, 2] <- names(News_coocs)[1:numberOfCoocs]
# Set the significances
News_tmpGraph[, 3] <- News_coocs[1:numberOfCoocs]

# Attach the triples to News_resultGraph
News_resultGraph <- rbind(News_resultGraph, News_tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newNews_coocTerm <- names(News_coocs)[i]
  News_coocs2 <- calculateCoocStatistics(newNews_coocTerm, News_binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  News_coocs2[1:10]
  
  # Structure of the temporary graph object
  News_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  News_tmpGraph[1:numberOfCoocs, 3] <- News_coocs2[1:numberOfCoocs]
  News_tmpGraph[, 1] <- newNews_coocTerm
  News_tmpGraph[, 2] <- names(News_coocs2)[1:numberOfCoocs]
  News_tmpGraph[, 3] <- News_coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  News_resultGraph <- rbind(News_resultGraph, News_tmpGraph[2:length(News_tmpGraph[, 1]), ])
}
```

As a result, resultGraph now contains all numberOfCoocs * numberOfCoocs edges of a term co-occurrence network.

### Visualization 

The package iGraph offers multiple graph visualizations for graph objects. Graph objects can be created from triple lists, such as those we just generated. In the next step we load the package iGraph and create a visualization of all nodes and edges from the object resultGraph.

```{r}
require(igraph)

# Set the graph and type
News_graphNetwork <- graph.data.frame(News_resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
News_graphVs <- V(News_graphNetwork)[degree(News_graphNetwork) < 2]
# These edges are removed from the graph
News_graphNetwork <- delete.vertices(News_graphNetwork, News_graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(News_graphNetwork)$color <- ifelse(V(News_graphNetwork)$name == News_coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
News_halfMaxSig <- max(E(News_graphNetwork)$sig) * 0.5
E(News_graphNetwork)$color <- ifelse(E(News_graphNetwork)$sig > News_halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(News_graphNetwork)$curved <- 0
# Size the nodes by their degree of networking
V(News_graphNetwork)$size <- log(degree(News_graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(News_graphNetwork)$size[V(News_graphNetwork)$size < 5] <- 4

# edge thickness
E(News_graphNetwork)$width <- 1

# Finaler Plot
News_network <- toVisNetworkData(News_graphNetwork)
visNetwork(nodes = News_network$nodes %>% 
             mutate(font.size = if_else(id == "coronavirus",25,15)),
           
           edges = News_network$edges %>% 
             mutate(color = NA), 
           
           main = "Corona keyword Co-occurrence", 
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE)

  
# visSave(file = "network.html")
```


## Twitter Network

### Load Token & Dictionary

```{r}
Corona_Dictionary <- read_csv("Corona_Dictionary2.csv")
  

Token_Twitter_network <- corpus_Twitter_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_replace(Corona_Dictionary$keyword, Corona_Dictionary$replace) %>% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```

### Load the DFM into the data

Again, we create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. An upper limit is not set (`Inf` = infinite).

Additionally, we are interested in the joint occurrence of words in a sentence. For this, we do not need the exact count of how often the terms occur, but only the information whether they occur together or not. This can be encoded in a binary document-term-matrix. The parameter `weighting` in the control options calls the `weightBin` function. This writes a 1 into the DTM if the term is contained in a sentence and 0 if not.

```{r}
# calculate multi-word unit candidates
Redit_collocations <- textstat_collocations(Token_Twitter_network, min_count = 50)

Redit_collocations <- tokens_compound(Token_Twitter_network, Redit_collocations)

minimumFrequency <- 10

Twitter_binDTM <- Redit_collocations %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")
```

### Counting co-occurrences

The counting of the joint word occurrence is easily possible via a matrix multiplication `(https://en.wikipedia.org/wiki/Matrix_multiplication)` on the binary DTM. For this purpose, the transposed matrix (dimensions: nTypes x nDocs) is multiplied by the original matrix (nDocs x nTypes), which as a result encodes a term-term matrix (dimensions: nTypes x nTypes).

```{r}
# Matrix multiplication for cooccurrence counts

Twitter_coocCounts <- t(Twitter_binDTM) %*% Twitter_binDTM

```

Let’s look at a snippet of the result. The matrix has nTerms rows and columns and is symmetric. Each cell contains the number of joint occurrences. In the diagonal, the frequencies of single occurrences of each term are encoded.

```{r}
as.matrix(coocCounts[202:205, 202:205])
```

Interprete as follows: agree appears together 7 times with opinion in the 62735 sentences of the SUTO addresses. agree alone occurs 310 times.

### Statistical significance

In order to not only count joint occurrence we have to determine their significance. Different significance-measures can be used. We need also various counts to calculate the significance of the joint occurrence of a term `i` (`coocTerm`) with any other term `j`: 

  * `k` - Number of all context units in the corpus 
  * `ki` - Number of occurrences of `coocTerm` 
  * `kj` - Number of occurrences of comparison term `j` 
  * `kij` - Number of joint occurrences of `coocTerm` and `j`

These quantities can be calculated for any term `coocTerm` as follows:

```{r}
Twitter_coocTerm <- c("coronavirus")
Twitter_k <- nrow(Twitter_binDTM)
Twitter_ki <- sum(Twitter_binDTM[, Twitter_coocTerm])
Twitter_kj <- colSums(Twitter_binDTM)
names(Twitter_kj) <- colnames(Twitter_binDTM)
Twitter_kij <- Twitter_coocCounts[Twitter_coocTerm, ]
```


An implementation in R for Mutual Information, Dice, and Log-Likelihood may look like this. At the end of each formula, the result is sorted so that the most significant co-occurrences are at the first ranks of the list.

```{r}
########## MI: log(Twitter_k*Twitter_kij / (Twitter_ki * Twitter_kj) ########
mutualInformationSig <- log(Twitter_k * Twitter_kij / (Twitter_ki * Twitter_kj))
mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]

########## DICE: 2 X&Y / X + Y ##############
dicesig <- 2 * Twitter_kij / (Twitter_ki + Twitter_kj)
dicesig <- dicesig[order(dicesig, decreasing=TRUE)]

########## Log Likelihood ###################
logsig <- 2 * ((Twitter_k * log(Twitter_k)) - (Twitter_ki * log(Twitter_ki)) - (Twitter_kj * log(Twitter_kj)) + (Twitter_kij * log(Twitter_kij)) 
               + (Twitter_k - Twitter_ki - Twitter_kj + Twitter_kij) * log(Twitter_k - Twitter_ki - Twitter_kj + Twitter_kij) 
               + (Twitter_ki - Twitter_kij) * log(Twitter_ki - Twitter_kij) + (Twitter_kj - Twitter_kij) * log(Twitter_kj - Twitter_kij) 
               - (Twitter_k - Twitter_ki) * log(Twitter_k - Twitter_ki) - (Twitter_k - Twitter_kj) * log(Twitter_k - Twitter_kj))


logsig <- logsig[order(logsig, decreasing=T)]
```

The result of the four variants for the statistical extraction of co-occurrence terms is shown in a data frame below. It can be seen that frequency is a bad indicator of meaning constitution. Mutual information emphasizes rather rare events in the data. Dice and Log-likelihood yield very well interpretable contexts.

```{r}
# Put all significance statistics in one Data-Frame
Twitter_resultOverView <- data.frame(
  names(sort(Twitter_kij, decreasing=T)[1:10]), sort(Twitter_kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)

colnames(Twitter_resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")

print(Twitter_resultOverView)
```

### Visualization of co-occurrence

In the following, we create a network visualization of significant co-occurrences.

For this, we provide the calculation of the co-occurrence significance measures, which we have just introduced, as single function in the file `calculateCoocStatistics.R`. This function can be imported into the current R-Session with the source command.


```{r}
# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
Twitter_coocTerm <- "coronavirus"
```

We use the imported function `calculateCoocStatistics` to calculate the co-occurrences for the target term “coronavirus”.

```{r}
Twitter_coocs <- calculateCoocStatistics(Twitter_coocTerm, Twitter_binDTM, measure="LOGLIK")

# Display the numberOfCoocs main terms
print(Twitter_coocs[1:numberOfCoocs])
```

To acquire an extended semantic environment of the target term, ‘secondary co-occurrence’ terms can be computed for each co-occurrence term of the target term. This results in a graph that can be visualized with special layout algorithms (e.g. Force Directed Graph).

Network graphs can be evaluated and visualized in R with the igraph-package. Any graph object can be created from a three-column data-frame. Each row in that data-frame is a triple. Each triple encodes an edge-information of two nodes (source, sink) and an edge-weight value.

For a term co-occurrence network, each triple consists of the target word, a co-occurring word and the significance of their joint occurrence. We denote the values with from, to, sig.

```{r}

Twitter_resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

```

The process of gathering the network for the target term runs in two steps. First, we obtain all significant co-occurrence terms for the target term. Second, we obtain all co-occurrences of the co-occurrence terms from step one.

Intermediate results for each term are stored as temporary triples named `tmpGraph.` With the `rbind` command (“row bind”, used for concatenation of data-frames) all `tmpGraph` are appended to the complete network object stored in `resultGraph.`

```{r}
# The structure of the temporary graph object is equal to that of the Twitter_resultGraph
Twitter_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
Twitter_tmpGraph[1:numberOfCoocs, 3] <- Twitter_coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
Twitter_tmpGraph[, 1] <- Twitter_coocTerm
# Entry of the co-occurrences into the second column of the respective line
Twitter_tmpGraph[, 2] <- names(Twitter_coocs)[1:numberOfCoocs]
# Set the significances
Twitter_tmpGraph[, 3] <- Twitter_coocs[1:numberOfCoocs]

# Attach the triples to Twitter_resultGraph
Twitter_resultGraph <- rbind(Twitter_resultGraph, Twitter_tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newTwitter_coocTerm <- names(Twitter_coocs)[i]
  Twitter_coocs2 <- calculateCoocStatistics(newTwitter_coocTerm, Twitter_binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  Twitter_coocs2[1:10]
  
  # Structure of the temporary graph object
  Twitter_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  Twitter_tmpGraph[1:numberOfCoocs, 3] <- Twitter_coocs2[1:numberOfCoocs]
  Twitter_tmpGraph[, 1] <- newTwitter_coocTerm
  Twitter_tmpGraph[, 2] <- names(Twitter_coocs2)[1:numberOfCoocs]
  Twitter_tmpGraph[, 3] <- Twitter_coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  Twitter_resultGraph <- rbind(Twitter_resultGraph, Twitter_tmpGraph[2:length(Twitter_tmpGraph[, 1]), ])
}
```

As a result, resultGraph now contains all numberOfCoocs * numberOfCoocs edges of a term co-occurrence network.

### Visualization 

The package iGraph offers multiple graph visualizations for graph objects. Graph objects can be created from triple lists, such as those we just generated. In the next step we load the package iGraph and create a visualization of all nodes and edges from the object resultGraph.

```{r}
require(igraph)

# Set the graph and type
Twitter_graphNetwork <- graph.data.frame(Twitter_resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
Twitter_graphVs <- V(Twitter_graphNetwork)[degree(Twitter_graphNetwork) < 2]
# These edges are removed from the graph
Twitter_graphNetwork <- delete.vertices(Twitter_graphNetwork, Twitter_graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(Twitter_graphNetwork)$color <- ifelse(V(Twitter_graphNetwork)$name == Twitter_coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
Twitter_halfMaxSig <- max(E(Twitter_graphNetwork)$sig) * 0.5
E(Twitter_graphNetwork)$color <- ifelse(E(Twitter_graphNetwork)$sig > Twitter_halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(Twitter_graphNetwork)$curved <- 0
# Size the nodes by their degree of networking
V(Twitter_graphNetwork)$size <- log(degree(Twitter_graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(Twitter_graphNetwork)$size[V(Twitter_graphNetwork)$size < 5] <- 4

# edge thickness
E(Twitter_graphNetwork)$width <- 1

# Finaler Plot
Twitter_network <- toVisNetworkData(Twitter_graphNetwork)
visNetwork(nodes = Twitter_network$nodes %>% 
             mutate(font.size = if_else(id == "coronavirus",25,15)),
           
           edges = Twitter_network$edges %>% 
             mutate(color = NA), 
           
           main = "Corona keyword Co-occurrence", 
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE)

  
# visSave(file = "network.html")
```


# Flow 4 - STM Topic Modeling

## Reddit data

### Convert object to stm

Convert dfm to stm model. 

```{r}

Reddit_stm <- convert(Reddit_DFM, to = "stm")

```


### Choosing `k` by parameter tuning

`https://juliasilge.com/blog/evaluating-stm/`

*Parallel processing*

```{r}
library(stm)
library(furrr)
plan(multiprocess)

Reddit_many_models <- data_frame(K = c(8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(Reddit_DFM, K = .,
                                               verbose = FALSE)))
```


*The model tuning*

```{r}
Reddit_heldout <- make.heldout(Reddit_DFM)

Reddit_k_result <- Reddit_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, Reddit_DFM),
         eval_heldout = map(topic_model, eval.heldout, Reddit_heldout$missing),
         residual = map(topic_model, checkResiduals, Reddit_DFM),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

Reddit_k_result
```

We’re evaluating things like the residuals, the semantic coherence of the topics, the likelihood for held-out datasets, and more. We can make some diagnostic plots using these quantities to understand how the models are performing at various numbers of topics. The following code makes a diagnostic plot similar to one that comes built in to the stm package.

```{r}
Reddit_k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.Reddit_heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 60")
```


The held-out likelihood is highest between 60 and 80, and the residuals are lowest around 60, so perhaps a good number of topics would be around there.

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff. Read more about semantic coherence in the original paper about it.


```{r}
Reddit_k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

### Run `stm` with the given K

`https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf`

In the example below, we enter in the variables additively, by allowing for the week variable, an integer variable measuring which day the blog was posted, to have a non-linear relationship in the topic estimation stage.

Additionally users can include more flexible functional forms of continuous covariates, including standard transforms like log(), as well as ns() or bs() from the splines package. The stm package also includes a convenience function s(), which selects a fairly flexible b-spline basis.

The argument init.type allows the user to specify an initialization method. The default choice, "Spectral", provides a deterministic initialization using the spectral algorithm given in Arora et al 2014. See Roberts, Stewart and Tingley (2016) for details and a comparison of different approaches. Particularly when the number of documents is relatively large we highly recommend the Spectral algorithm which often performs extremely well.


```{r}

Reddit_topic_model <- stm(
  documents = Reddit_stm$documents, 
  vocab = Reddit_stm$vocab,
  K = 25,
  prevalence =~ s(week),
  data = Reddit_stm$meta,
  init.type = "Spectral",
  seed = 123456)

```


### Listing top words for selected topics 

`https://github.com/dondealban/learning-stm`

`[For reference]` Listing top words for selected topics.

```{r}

labelTopics <- labelTopics(Reddit_topic_model)

```


### Show document contain the topic

`https://github.com/dondealban/learning-stm`

`[For reference]` Using findThoughts() function reads documents that are highly correlated with the user-specified topics. Object 'thoughts1' contains 3 documents about topic #3 and 'texts=shortdoc' gives just the first 250 words.

```{r}

thoughts3 <- findThoughts(Reddit_topic_model, texts=shortdoc, n=3, topics=3)$docs[[1]]
plotQuote(thoughts3, width=40, main="Topic 3")

```


### Topic Quality

`http://thomaselliott.me/pdfs/earl/topic_modeling.html`

Check if the K we choose is good enough, the topic quality is based on these KPI

`Semantic coherence` - empirical cooccurence of terms with high probability under a given topic. Given term l appears, what is the probability that term m will also appear? For the topic, coherence is the sum of the log of these probabilities.

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together. Mimno et al. (2011) show that the metric correlates well with human judgment of topic quality. Formally, let D(v,v′) be the number of times that words v and v′ appear together in a document. Then for a list of the M most probable words in topic k, the semantic coherence for topic k is given as. In Roberts et al. (2014) we noted that attaining high semantic coherence was relatively easy by having a few topics dominated by very common words. We thus propose to measure topic quality through a combination of semantic coherence and exclusivity of words to topics.

`Exclusivity` - top words for a topic are unlikely to appear in top words of other topics. Essentially the rate of a word in a topic divided by the sum of the rate of the word in all other topics.

[This analysis give us a hint if topics are similar]


```{r}

topicQuality(model=Reddit_topic_model, documents=Reddit_stm$documents)

```


### Correlations between topics

`https://github.com/dondealban/learning-stm`

Finally, for topicCorr() an STM permits correlations between topics. Positive correlations between topics indicate that both topics are likely to be discussed within a document. A graphical network display shows how closely related topics are to one another (i.e., how likely they are to appear in the same document). This function requires igraph R package.


```{r}
mod.out.corr <- topicCorr(Reddit_topic_model)
plot(mod.out.corr)
```


### Interactive visualisation

`https://github.com/dondealban/learning-stm`

`[For reference]`

Finally, the stmCorrViz() function for the package of the same name generates an interactive visualisation of topic hierarchy/correlations in a structural topicl model. The package performs a hierarchical clustering of topics that are then exported to a JSON object and visualised using D3.


```{r}
stmCorrViz(Reddit_topic_model, "stm-interactive-correlation.html", 
           documents_raw=data$documents, documents_matrix=out$documents)
```


## Twitter data

### Convert object to stm

Convert dfm to stm model. 

```{r}

Twitter_stm <- convert(Twitter_DFM, to = "stm")

```


### Choosing `k` by parameter tuning

`https://juliasilge.com/blog/evaluating-stm/`

*Parallel processing*

```{r}
library(stm)
library(furrr)
plan(multiprocess)

Twitter_many_models <- data_frame(K = c(8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(Twitter_DFM, K = .,
                                               verbose = FALSE)))
```


*The model tuning*

```{r}
Twitter_heldout <- make.heldout(Twitter_DFM)

Twitter_k_result <- Twitter_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, Twitter_DFM),
         eval_heldout = map(topic_model, eval.heldout, Twitter_heldout$missing),
         residual = map(topic_model, checkResiduals, Twitter_DFM),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

Twitter_k_result
```

We’re evaluating things like the residuals, the semantic coherence of the topics, the likelihood for held-out datasets, and more. We can make some diagnostic plots using these quantities to understand how the models are performing at various numbers of topics. The following code makes a diagnostic plot similar to one that comes built in to the stm package.

```{r}
Twitter_k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 12")
```


The held-out likelihood is highest between 60 and 80, and the residuals are lowest around 60, so perhaps a good number of topics would be around there.

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff. Read more about semantic coherence in the original paper about it.


```{r}
Twitter_k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

### Run `stm` with the given K

`https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf`

In the example below, we enter in the variables additively, by allowing for the week variable, an integer variable measuring which day the blog was posted, to have a non-linear relationship in the topic estimation stage.

Additionally users can include more flexible functional forms of continuous covariates, including standard transforms like log(), as well as ns() or bs() from the splines package. The stm package also includes a convenience function s(), which selects a fairly flexible b-spline basis.

The argument init.type allows the user to specify an initialization method. The default choice, "Spectral", provides a deterministic initialization using the spectral algorithm given in Arora et al 2014. See Roberts, Stewart and Tingley (2016) for details and a comparison of different approaches. Particularly when the number of documents is relatively large we highly recommend the Spectral algorithm which often performs extremely well.


```{r}

Twitter_topic_model <- stm(
  documents = Twitter_stm$documents, 
  vocab = Twitter_stm$vocab,
  K = 12,
  prevalence =~ s(week),
  data = Twitter_stm$meta,
  init.type = "Spectral",
  seed = 123456)

```


### Listing top words for selected topics 

`https://github.com/dondealban/learning-stm`

`[For reference]` Listing top words for selected topics.

```{r}

labelTopics <- labelTopics(Twitter_topic_model)

```


### Show document contain the topic

`https://github.com/dondealban/learning-stm`

`[For reference]` Using findThoughts() function reads documents that are highly correlated with the user-specified topics. Object 'thoughts1' contains 3 documents about topic #3 and 'texts=shortdoc' gives just the first 250 words.

```{r}

thoughts3 <- findThoughts(Twitter_topic_model, texts=shortdoc, n=3, topics=3)$docs[[1]]
plotQuote(thoughts3, width=40, main="Topic 3")

```


### Topic Quality

`http://thomaselliott.me/pdfs/earl/topic_modeling.html`

Check if the K we choose is good enough, the topic quality is based on these KPI

`Semantic coherence` - empirical cooccurence of terms with high probability under a given topic. Given term l appears, what is the probability that term m will also appear? For the topic, coherence is the sum of the log of these probabilities.

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together. Mimno et al. (2011) show that the metric correlates well with human judgment of topic quality. Formally, let D(v,v′) be the number of times that words v and v′ appear together in a document. Then for a list of the M most probable words in topic k, the semantic coherence for topic k is given as. In Roberts et al. (2014) we noted that attaining high semantic coherence was relatively easy by having a few topics dominated by very common words. We thus propose to measure topic quality through a combination of semantic coherence and exclusivity of words to topics.

`Exclusivity` - top words for a topic are unlikely to appear in top words of other topics. Essentially the rate of a word in a topic divided by the sum of the rate of the word in all other topics.

[This analysis give us a hint if topics are similar]


```{r}

topicQuality(model=Twitter_topic_model, documents=Twitter_stm$documents)

```


### Correlations between topics

`https://github.com/dondealban/learning-stm`

Finally, for topicCorr() an STM permits correlations between topics. Positive correlations between topics indicate that both topics are likely to be discussed within a document. A graphical network display shows how closely related topics are to one another (i.e., how likely they are to appear in the same document). This function requires igraph R package.


```{r}
mod.out.corr <- topicCorr(Twitter_topic_model)
plot(mod.out.corr)
```


### Interactive visualisation

`https://github.com/dondealban/learning-stm`

`[For reference]`

Finally, the stmCorrViz() function for the package of the same name generates an interactive visualisation of topic hierarchy/correlations in a structural topicl model. The package performs a hierarchical clustering of topics that are then exported to a JSON object and visualised using D3.


```{r}
stmCorrViz(Twitter_topic_model, "stm-interactive-correlation.html", 
           documents_raw=data$documents, documents_matrix=out$documents)
```


## News Data

### Convert object to stm

Convert dfm to stm model. 

```{r}

News_stm <- convert(News_DFM, to = "stm")

```


### Choosing `k` by parameter tuning

`https://juliasilge.com/blog/evaluating-stm/`

*Parallel processing*

```{r}
library(stm)
library(furrr)
plan(multiprocess)

News_many_models <- data_frame(K = c(8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(News_DFM, K = .,
                                               verbose = FALSE)))
```


*The model tuning*

```{r}
News_heldout <- make.heldout(News_DFM)

News_k_result <- News_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, News_DFM),
         eval_heldout = map(topic_model, eval.heldout, News_heldout$missing),
         residual = map(topic_model, checkResiduals, News_DFM),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

News_k_result
```

We’re evaluating things like the residuals, the semantic coherence of the topics, the likelihood for held-out datasets, and more. We can make some diagnostic plots using these quantities to understand how the models are performing at various numbers of topics. The following code makes a diagnostic plot similar to one that comes built in to the stm package.

```{r}
News_k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics - News data",
       subtitle = "These diagnostics indicate that a good number of topics would be around 19")
```


The held-out likelihood is highest between 60 and 80, and the residuals are lowest around 60, so perhaps a good number of topics would be around there.

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and it’s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. It’s a tradeoff. Read more about semantic coherence in the original paper about it.


```{r}
News_k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence - News data",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

### Run `stm` with the given K

`https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf`

In the example below, we enter in the variables additively, by allowing for the week variable, an integer variable measuring which day the blog was posted, to have a non-linear relationship in the topic estimation stage.

Additionally users can include more flexible functional forms of continuous covariates, including standard transforms like log(), as well as ns() or bs() from the splines package. The stm package also includes a convenience function s(), which selects a fairly flexible b-spline basis.

The argument init.type allows the user to specify an initialization method. The default choice, "Spectral", provides a deterministic initialization using the spectral algorithm given in Arora et al 2014. See Roberts, Stewart and Tingley (2016) for details and a comparison of different approaches. Particularly when the number of documents is relatively large we highly recommend the Spectral algorithm which often performs extremely well.


```{r}

News_topic_model <- stm(
  documents = News_stm$documents, 
  vocab = News_stm$vocab,
  K = 19,
  prevalence =~ s(week),
  data = News_stm$meta,
  init.type = "Spectral",
  seed = 123456)

```


### Listing top words for selected topics 

`https://github.com/dondealban/learning-stm`

`[For reference]` Listing top words for selected topics.

```{r}

labelTopics <- labelTopics(News_topic_model)

```


### Show document contain the topic

`https://github.com/dondealban/learning-stm`

`[For reference]` Using findThoughts() function reads documents that are highly correlated with the user-specified topics. Object 'thoughts1' contains 3 documents about topic #3 and 'texts=shortdoc' gives just the first 250 words.

```{r}

thoughts3 <- findThoughts(News_topic_model, texts=shortdoc, n=3, topics=3)$docs[[1]]
plotQuote(thoughts3, width=40, main="Topic 3")

```


### Topic Quality

`http://thomaselliott.me/pdfs/earl/topic_modeling.html`

Check if the K we choose is good enough, the topic quality is based on these KPI

`Semantic coherence` - empirical cooccurence of terms with high probability under a given topic. Given term l appears, what is the probability that term m will also appear? For the topic, coherence is the sum of the log of these probabilities.

Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together. Mimno et al. (2011) show that the metric correlates well with human judgment of topic quality. Formally, let D(v,v′) be the number of times that words v and v′ appear together in a document. Then for a list of the M most probable words in topic k, the semantic coherence for topic k is given as. In Roberts et al. (2014) we noted that attaining high semantic coherence was relatively easy by having a few topics dominated by very common words. We thus propose to measure topic quality through a combination of semantic coherence and exclusivity of words to topics.

`Exclusivity` - top words for a topic are unlikely to appear in top words of other topics. Essentially the rate of a word in a topic divided by the sum of the rate of the word in all other topics.

[This analysis give us a hint if topics are similar]


```{r}

topicQuality(model=News_topic_model, documents=News_stm$documents)

```


### Correlations between topics

`https://github.com/dondealban/learning-stm`

Finally, for topicCorr() an STM permits correlations between topics. Positive correlations between topics indicate that both topics are likely to be discussed within a document. A graphical network display shows how closely related topics are to one another (i.e., how likely they are to appear in the same document). This function requires igraph R package.


```{r}
mod.out.corr <- topicCorr(News_topic_model)
plot(mod.out.corr)
```


### Interactive visualisation

`https://github.com/dondealban/learning-stm`

`[For reference]`

Finally, the stmCorrViz() function for the package of the same name generates an interactive visualisation of topic hierarchy/correlations in a structural topicl model. The package performs a hierarchical clustering of topics that are then exported to a JSON object and visualised using D3.


```{r}
stmCorrViz(News_topic_model, "stm-interactive-correlation.html", 
           documents_raw=data$documents, documents_matrix=out$documents)
sum(bing$sentiment == "positive")
```



# Flow 5 - Topic Model Sentiments

To investigate sentiments in news articles, reddit comments and tweets, we first split a document into tokens. We calculated sentiment scores with the opinion lexicon developed by Bing Liu et al., which is included in the tidytext package. The Bing lexicon is a list of English words categorized into positive and negative categories. It comprises 2,005 positive terms and 4,781 negative terms.

We assigned the sentiment value for each topic word with an integer of +1 for positive and -1 for negative, and the probability of a word in a topic given by the STM topic model as the term weight of the word. As a result, topic sentiment score is calculated as
$$Sentiment\ Score\ of\ Topic\ A = \sum_{i=1}^{n} (prob(word_i|Topic_A)*sentiment\ value(word_i))$$

An overall topic sentiment score is computed by multiplying the sentiment value by the probability of words and summing the products in a chosen topic. Multiple topics could have the same words with different probability values. Therefore, the sentiment score of a topic would be distinguished from others even if the same set of words appear in different topics. We then determined the weekly sentiment score of a topic by multiplying the overall score by the proportion of the topic in each observed week.

## Reddit Data

### Extract beta and gamma matrix to data frame format

```{r}
Reddit_beta <- tidy(Reddit_topic_model, matrix = "beta")
Reddit_gamma <- tidy(Reddit_topic_model, matrix = "gamma", 
                   document_names = reddit_data$week) %>% 
  group_by(document, topic) %>% #calculate the total gamma per topic per week
  summarise(total_gamma = sum(gamma))
```


### Get topic proportion per week and extract the top topic

```{r}
names(Reddit_gamma)[names(Reddit_gamma) == "document"] <- "week"

Reddit_topic_proportion <- left_join(Reddit_gamma, plot_reddit, by = "week") %>% 
  mutate(topic_proportion = total_gamma / n) %>% 
  arrange(desc(topic_proportion)) %>%
  group_by(week) %>% 
  slice(seq_len(1)) %>%
  ungroup() %>%
  select(-c("total_gamma", "n"))
```


### Using Bing lexicon, make a data frame with the sentiment for each word  

```{r}
bing <- get_sentiments("bing")
Reddit_sc <- inner_join(Reddit_beta, bing, by = c("term" = "word")) %>% 
  mutate(score = ifelse(sentiment == "negative", beta * (-1), beta)) %>% 
  group_by(topic, sentiment) %>% 
  summarise(sentiment_score = sum(score))
```


### Combine topic proportion data frame and word sentiment data frame. 

```{r}
Reddit_top_topic_sentiment <- left_join(Reddit_topic_proportion, 
                                      Reddit_sc, by = "topic") %>% 
  mutate(topic_score = topic_proportion * sentiment_score) %>% 
  group_by(week, topic, source) %>% 
  summarise(polarity = sum(topic_score))
```


## Twitter Data


```{r}
Twitter_beta <- tidy(Twitter_topic_model, matrix = "beta")
Twitter_gamma <- tidy(Twitter_topic_model, matrix = "gamma", 
                   document_names = tweets_data$week) %>% 
  group_by(document, topic) %>% #calculate the total gamma per topic per week
  summarise(total_gamma = sum(gamma))
```


### Get topic proportion per week and extract the top topic

```{r}
names(Twitter_gamma)[names(Twitter_gamma) == "document"] <- "week"

Twitter_topic_proportion <- left_join(Twitter_gamma, plot_tweets, by = "week") %>% 
  mutate(topic_proportion = total_gamma / n) %>% 
  arrange(desc(topic_proportion)) %>%
  group_by(week) %>% 
  slice(seq_len(1)) %>%
  ungroup() %>%
  select(-c("total_gamma", "n"))
```


### Using Bing lexicon, make a data frame with the sentiment for each word  

```{r}
bing <- get_sentiments("bing")
Twitter_sc <- inner_join(Twitter_beta, bing, by = c("term" = "word")) %>% 
  mutate(score = ifelse(sentiment == "negative", beta * (-1), beta)) %>% 
  group_by(topic, sentiment) %>% 
  summarise(sentiment_score = sum(score))
```


### Combine topic proportion data frame and word sentiment data frame. 

```{r}
Twitter_top_topic_sentiment <- left_join(Twitter_topic_proportion, 
                                      Twitter_sc, by = "topic") %>% 
  mutate(topic_score = topic_proportion * sentiment_score) %>% 
  group_by(week, topic, source) %>% 
  summarise(polarity = sum(topic_score))
```


## News Data

### Extract beta and gamma matrix to data frame format

```{r}
News_beta <- tidy(News_topic_model, matrix = "beta")
News_gamma <- tidy(News_topic_model, matrix = "gamma", 
                   document_names = news_data$week) %>% 
  group_by(document, topic) %>% #calculate the total gamma per topic per week
  summarise(total_gamma = sum(gamma))
```


### Get topic proportion per week and extract the top topic

```{r}
names(News_gamma)[names(News_gamma) == "document"] <- "week"

News_topic_proportion <- left_join(News_gamma, plot_news, by = "week") %>% 
  mutate(topic_proportion = total_gamma / n) %>% 
  arrange(desc(topic_proportion)) %>%
  group_by(week) %>% 
  slice(seq_len(1)) %>%
  ungroup() %>%
  select(-c("total_gamma", "n"))
```


### Using Bing lexicon, make a data frame with the sentiment for each word  

```{r}
bing <- get_sentiments("bing")
News_sc <- inner_join(News_beta, bing, by = c("term" = "word")) %>% 
  mutate(score = ifelse(sentiment == "negative", beta * (-1), beta)) %>% 
  group_by(topic, sentiment) %>% 
  summarise(sentiment_score = sum(score))
```


### Combine topic proportion data frame and word sentiment data frame. 

```{r}
News_top_topic_sentiment <- left_join(News_topic_proportion, 
                                         News_sc, by = "topic") %>% 
  mutate(topic_score = topic_proportion * sentiment_score) %>% 
  group_by(week, topic, source) %>% 
  summarise(polarity = sum(topic_score))
```


## Plot the changes in top topics in all three sources along with their sentiment scores

```{r}
library(ggrepel)

All_source_sc <- rbind(Reddit_top_topic_sentiment, Twitter_top_topic_sentiment, News_top_topic_sentiment)

All_source_sc %>% 
  ggplot(aes(x = week, y = polarity, color = source)) +
  geom_point(size = 1.2) +
  geom_path(size = 0.7, linetype = "dashed") +
  geom_text_repel(aes(label = topic),
                  nudge_x = -0.05,
                  direction = "y",
                  size = 4,
                  show.legend = FALSE) +
  geom_hline(yintercept = 0, color = "bisque4", 
             size = 0.6, alpha = 0.6) +
  labs(x = "Week", y = "Sentiment Score") + 
  scale_x_continuous(breaks = News_top_topic_sentiment$week,
                     labels = News_top_topic_sentiment$week) +
  scale_y_continuous(limits = c(-0.035, 0.015),
                     n.breaks = 10) +
  #theme(legend.position = "bottom") +
  #scale_colour_manual(values = c("#1F78B4", "#E3211C", "#33A02B")) +
  theme(panel.grid.major.x = element_blank(), #remove vertical gridlines
        legend.position = "bottom")
```
















