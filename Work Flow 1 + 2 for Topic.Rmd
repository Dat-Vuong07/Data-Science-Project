---
title: "Work Flow Data Science Project"
author: "Dat Quoc Vuong"
date: "5/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, include = F)
```

Package

```{r}
library(tidyverse)
library(quanteda)
library(tidytext)
library(tm)
library(caret)
library(readtext)
library(tokenizers)
library(lubridate)
library(quanteda.dictionaries)
library(qdapRegex)
library(topicmodels)
library(textmineR)
library(stm)
library(stmCorrViz)
pacman::p_load("furrr")
```


# Flow 1 - Load + Clean data

## Reddit

```{r}

reddit_data <- read_rds("Reddit_Data.rds") %>%
  filter(!week == "17" )

```

## Twitter

```{r}

tweets <- read_csv("twitter_coronavirus.csv")

# Transform the date column

tweets_data <- tweets %>% 
  separate(created_at, c("wday", "month", "day", "time", "plus", "year"), 
           sep = " ") %>% 
  mutate(date = paste(month, day, year, sep = " ")) %>% 
  select(c("text", "date"))

tweets_data$date <- as.Date(tweets_data$date, format = "%b %d %Y")
tweets_data <- tweets_data %>% 
  mutate(week = isoweek(date)) %>% 
  filter(week > 2)

# Clean hashtags, links and special characters

tweets_data$text <- tweets_data$text %>% 
  str_replace_all("#", " ") %>%
  str_remove_all("(?<=^|\\s)http[^\\s]+") %>% 
  str_remove_all("[^a-zA-Z0-9 ]") %>% 
  trimws()

# Remove blank text after cleaning

tweets_data <- tweets_data %>% 
  filter(str_count(text, pattern = boundary("word")) > 0)
```


## News

```{r}

news_data <- read_csv("news_coronavirus.csv")

# Add week and remove unnecessary column

news_data <- news_data %>% 
  mutate(week = isoweek(publish_date)) %>% 
  select(-title)
```


# Flow 2 - Copus - Token

## Reddit

### Create a copus 

```{r}

corpus_Reddit_data <- corpus(reddit_data, text_field = c("comments"), unique_docnames = F) %>%
  `docnames<-`(reddit_data$week)

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")
```


### Create Token

```{r}

Token_Reddit_data <- corpus_Reddit_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_remove(pattern = c("subreddit", "subredditmessagecomposetorcoronavirusus", "652000", "discussion", "thread", "subreddits", "people")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

We keep only the top 5% of the most frequent features (min_termfreq = 0.95) that appear in less than 10% of all documents (max_docfreq = 0.1) using dfm_trim() to focus on common but distinguishing features.

```{r}

Reddit_DFM <- Token_Reddit_data %>% 
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

Reddit_DFM <- Reddit_DFM[ntoken(Reddit_DFM) > 0,]
```


## Twitter

### Create a copus 

```{r}

corpus_Twitter_data <- corpus(tweets_data, text_field = c("text"),
                             unique_docnames = F) %>%
  `docnames<-`(tweets_data$week)

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")
```


### Create Token

```{r}

Token_Twitter_data <- corpus_Twitter_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = FALSE) %>%
  tokens_remove(pattern = c("people", "covid19", "covid")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, 
                 valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

We keep only the top 5% of the most frequent features (min_termfreq = 0.95) that appear in less than 10% of all documents (max_docfreq = 0.1) using dfm_trim() to focus on common but distinguishing features.

```{r}

Twitter_DFM <- Token_Twitter_data %>% 
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

Twitter_DFM <- Twitter_DFM[ntoken(Twitter_DFM) > 0,]
```


## News

### Create a corpus 

```{r}

corpus_News_data <- corpus(news_data, text_field = c("text"), 
                           unique_docnames = F) %>%
  `docnames<-`(news_data$week) # update the name of document in the copus

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")

```

### Create Tokens

```{r}

Token_news_data <- corpus_News_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, 
         remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_remove(pattern = c("amid", "updates", "live", "video", "briefing")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, 
                 valuetype = "glob") %>%
  tokens_replace(pattern = c("covid-19", "ncov", "cov"), 
                 c("coronavirus", "coronavirus", "coronavirus"),
                 valuetype = "fixed") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

We keep only the top 5% of the most frequent features (min_termfreq = 0.95) that appear in less than 10% of all documents (max_docfreq = 0.1) using dfm_trim() to focus on common but distinguishing features.

```{r}

News_DFM <- Token_news_data %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

News_DFM <- News_DFM[ntoken(News_DFM) > 0, ]
#dim(news_DFM)

```


